
# coding: utf-8

# ---
# 
# _You are currently looking at **version 1.2** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-social-network-analysis/resources/yPcBs) course resource._
# 
# ---

# # Assignment 4

# In[38]:


import networkx as nx
import pandas as pd
import numpy as np
import pickle


# ---
# 
# ## Part 1 - Random Graph Identification
# 
# For the first part of this assignment you will analyze randomly generated graphs and determine which algorithm created them.

# In[2]:


P1_Graphs = pickle.load(open('A4_graphs','rb'))


# <br>
# `P1_Graphs` is a list containing 5 networkx graphs. Each of these graphs were generated by one of three possible algorithms:
# * Preferential Attachment (`'PA'`)
# * Small World with low probability of rewiring (`'SW_L'`)
# * Small World with high probability of rewiring (`'SW_H'`)
# 
# Anaylze each of the 5 graphs and determine which of the three algorithms generated the graph.
# 
# *The `graph_identification` function should return a list of length 5 where each element in the list is either `'PA'`, `'SW_L'`, or `'SW_H'`.*

# In[3]:


def graph_identification():
    
    # Your Code Here
    
    return ['PA', 'SW_L', 'SW_L', 'PA','SW_H']
#count = 1
#for G in P1_Graphs:
#    print('Graph'+ str(count))
##    degrees = G.degree()
#    degree_values = sorted(set(degrees.values()))
#    histogram = [list(degrees.values()).count(i)/float(nx.number_of_nodes(G)) for i in degree_values]
#    print (histogram)
#    print(np.sum(histogram))
#    print(nx.average_clustering(G))
#    print(nx.average_shortest_path_length(G))
#    count += 1


# ---
# 
# ## Part 2 - Company Emails
# 
# For the second part of this assignment you will be workking with a company's email network where each node corresponds to a person at the company, and each edge indicates that at least one email has been sent between two people.
# 
# The network also contains the node attributes `Department` and `ManagementSalary`.
# 
# `Department` indicates the department in the company which the person belongs to, and `ManagementSalary` indicates whether that person is receiving a management position salary.

# In[4]:


G = nx.read_gpickle('email_prediction.txt')

print(nx.info(G))


# ### Part 2A - Salary Prediction
# 
# Using network `G`, identify the people in the network with missing values for the node attribute `ManagementSalary` and predict whether or not these individuals are receiving a management position salary.
# 
# To accomplish this, you will need to create a matrix of node features using networkx, train a sklearn classifier on nodes that have `ManagementSalary` data, and predict a probability of the node receiving a management salary for nodes where `ManagementSalary` is missing.
# 
# 
# 
# Your predictions will need to be given as the probability that the corresponding employee is receiving a management position salary.
# 
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# 
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
# 
# Using your trained classifier, return a series of length 252 with the data being the probability of receiving management salary, and the index being the node id.
# 
#     Example:
#     
#         1       1.0
#         2       0.0
#         5       0.8
#         8       1.0
#             ...
#         996     0.7
#         1000    0.5
#         1001    0.0
#         Length: 252, dtype: float64

# In[5]:


def salary_predictions():
    df = pd.DataFrame(index=G.nodes())
    df['Department'] = pd.Series(nx.get_node_attributes(G, 'Department'))
    df['ManagementSalary']=pd.Series(nx.get_node_attributes(G, 'ManagementSalary'))
    df['degree'] = pd.Series(G.degree())
    df['degree_centrality'] = pd.Series(nx.degree_centrality(G))
    df['closeness_centrality'] = pd.Series(nx.closeness_centrality(G))
    df['betweenness_centrality'] = pd.Series(nx.betweenness_centrality(G))
    df1 = df.dropna(axis=0)
    df1 = df.dropna(axis=0)
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.metrics import roc_curve, auc
    features1 = df1[['Department','degree', 'degree_centrality', 'closeness_centrality', 'betweenness_centrality']]
    target1 = df1['ManagementSalary']
    target1 = target1.astype(int)
    clf = RandomForestClassifier(n_estimators=80).fit(features1, target1)
    df2 = df[df['ManagementSalary'].isnull()]
    features2 = df2[['Department','degree', 'degree_centrality', 'closeness_centrality', 'betweenness_centrality']]
    
    y_score_clf = clf.predict_proba(features2)
    y_score_clf1 = [i[1] for i in y_score_clf]
    answer_series = pd.Series(y_score_clf1, index=df2.index)
    
    return answer_series

#df = pd.DataFrame(index=G.nodes())
#df['Department'] = pd.Series(nx.get_node_attributes(G, 'Department'))
#df['ManagementSalary']=pd.Series(nx.get_node_attributes(G, 'ManagementSalary'))
#df['degree'] = pd.Series(G.degree())
#df['degree_centrality'] = pd.Series(nx.degree_centrality(G))
#df['closeness_centrality'] = pd.Series(nx.closeness_centrality(G))
#df['betweenness_centrality'] = pd.Series(nx.betweenness_centrality(G))

                                    

#df1 = df.dropna(axis=0)
#df1['ManagementSalary'] = df1['ManagementSalary'].astype('int')
#from sklearn.model_selection import train_test_split
#from sklearn.ensemble import RandomForestClassifier
#from sklearn.metrics import roc_curve, auc
#features = df1[['Department','degree', 'degree_centrality', 'closeness_centrality', 'betweenness_centrality']]
#target = df1['ManagementSalary']
#target = target.astype(int)
#X_train, X_test, y_train, y_test = train_test_split(features, target)
#clf = RandomForestClassifier(n_estimators=80).fit(X_train, y_train.astype('int'))
#y_score_clf = clf.predict_proba(X_test)
#y_score_clf1 = [i[1] for i in y_score_clf]
#fpr, tpr, _ = roc_curve(y_test, y_score_clf1)
#auc(fpr, tpr)
#len(roc_curve(y_test, y_score_clf1))
#y_score_clf
#df[df['ManagementSalary'].isnull()]
salary_predictions()


# In[6]:


#nx.betweenness_centrality(G)


# ### Part 2B - New Connections Prediction
# 
# For the last part of this assignment, you will predict future connections between employees of the network. The future connections information has been loaded into the variable `future_connections`. The index is a tuple indicating a pair of nodes that currently do not have a connection, and the `Future Connection` column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection.

# In[45]:


future_connections = pd.read_csv('Future_Connections.csv', index_col=0, converters={0: eval})
future_connections.head(10)


# Using network `G` and `future_connections`, identify the edges in `future_connections` with missing values and predict whether or not these edges will have a future connection.
# 
# To accomplish this, you will need to create a matrix of features for the edges found in `future_connections` using networkx, train a sklearn classifier on those edges in `future_connections` that have `Future Connection` data, and predict a probability of the edge being a future connection for those edges in `future_connections` where `Future Connection` is missing.
# 
# 
# 
# Your predictions will need to be given as the probability of the corresponding edge being a future connection.
# 
# The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).
# 
# Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).
# 
# Using your trained classifier, return a series of length 122112 with the data being the probability of the edge being a future connection, and the index being the edge as represented by a tuple of nodes.
# 
#     Example:
#     
#         (107, 348)    0.35
#         (542, 751)    0.40
#         (20, 426)     0.55
#         (50, 989)     0.35
#                   ...
#         (939, 940)    0.15
#         (555, 905)    0.35
#         (75, 101)     0.65
#         Length: 122112, dtype: float64

# In[53]:


def new_connections_predictions():
    #df_target = future_connections[future_connections['Future Connection'].isnull()]
    #df_graph = future_connections.dropna(axis=0)
    #df_graph1 = df_graph[0::10] # this controls how many samples I take
    df = future_connections
    edge_list = list(df.index)
    #edge_list_short = list(df_graph1.index)
    G = nx.Graph()
    G.add_edges_from(edge_list)
    G.edges()
    df['preferential_attachment'] = [i[2]for i in list(nx.preferential_attachment(G, edge_list))]
    #df['common_neighbors'] = [len(list(nx.common_neighbors(G,e[0],e[1]))) for e in edge_list]
    #df['jaccard_coefficient'] = [i[2] for i in list(nx.jaccard_coefficient(G, edge_list))]
    #df['resource_allocation'] = [i[2] for i in list(nx.resource_allocation_index(G, edge_list))]
    #df['future_connection'] = df_graph['Future Connection']
    df_target = df[df['Future Connection'].isnull()]
    df_train = df.dropna(axis=0)
    X_train = df_train[['preferential_attachment', 'common_neighbors', 'jaccard_coefficient', 'resource_allocation']]
    y_train = df_train['Future Connection']
    X_test = df_target[['preferential_attachment', 'common_neighbors', 'jaccard_coefficient', 'resource_allocation']]
    y_test = df_target['Future Connection']
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import MinMaxScaler
    Scaler = MinMaxScaler()
    Scaler.fit(X_train)
    X_train_scaled = Scaler.transform(X_train)
    X_test_scaled = Scaler.transform(X_test)
    lr = LogisticRegression(C=10000, penalty='l2').fit(X_train_scaled,y_train)
    prob_array = lr.predict_proba(X_test_scaled)
    positive_prob_list = [i[1] for i in prob_array]

    
    
    return pd.Series(positive_prob_list, index = df_target.index)
new_connections_predictions()
#df_target = future_connections[future_connections.isnull()]
#df_graph = future_connections.dropna(axis=0)
#df_graph1 = df_graph[0::10] # this controls how many samples I take
#edge_list = list(df_graph.index)
#edge_list_short = list(df_graph1.index)
#G = nx.Graph()
#G.add_edges_from(edge_list)
#G.edges()
#df = pd.DataFrame(index=edge_list_short)
#df['preferential_attachment'] = [i[2]for i in list(nx.preferential_attachment(G, edge_list_short))]
#df['common_neighbors'] = [len(list(nx.common_neighbors(G,e[0],e[1]))) for e in edge_list_short]
#df['jaccard_coefficient'] = [i[2] for i in list(nx.jaccard_coefficient(G, edge_list_short))]
#df['resource_allocation'] = [i[2] for i in list(nx.resource_allocation_index(G, edge_list_short))]
#df['future_connection'] = df_graph1['Future Connection']

